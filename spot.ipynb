{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "# import dataset, network to train and metric to optimize\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, RecurrentNetwork, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data.examples import get_stallion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load data: this is pandas dataframe with at least a column for\n",
    "# * the target (what you want to predict)\n",
    "# * the timeseries ID (which should be a unique string to identify each timeseries)\n",
    "# * the time of the observation (which should be a monotonically increasing integer)\n",
    "TRANSFORM_DATA = True\n",
    "if TRANSFORM_DATA:\n",
    "    raw_data = pd.read_csv('spotData.csv')\n",
    "    date_columns = [x for x in raw_data.columns if x.startswith('2021')]\n",
    "    not_date_columns = [x for x in raw_data.columns if not x.startswith('2021')]\n",
    "    raw_data = raw_data.melt(id_vars=not_date_columns, value_vars=date_columns, var_name='date',\n",
    "                             value_name='blabla').drop(columns=['blabla'])\n",
    "    raw_data.to_csv('data.csv', index=False)\n",
    "else:\n",
    "    raw_data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              Region instanceType major    minor               Type     OS  \\\n0            us-east    a1.medium    a1   medium  generalCurrentGen  linux   \n2            us-east     a1.large    a1    large  generalCurrentGen  linux   \n4            us-east    a1.xlarge    a1   xlarge  generalCurrentGen  linux   \n6            us-east   a1.2xlarge    a1  2xlarge  generalCurrentGen  linux   \n8            us-east   a1.4xlarge    a1  4xlarge  generalCurrentGen  linux   \n...              ...          ...   ...      ...                ...    ...   \n16059069  eu-south-1    m5d.metal   m5d    metal    hiMemCurrentGen  mswin   \n16059070  eu-south-1     r5.metal    r5    metal    hiMemCurrentGen  linux   \n16059071  eu-south-1     r5.metal    r5    metal    hiMemCurrentGen  mswin   \n16059072  eu-south-1    r5d.metal   r5d    metal    hiMemCurrentGen  linux   \n16059073  eu-south-1    r5d.metal   r5d    metal    hiMemCurrentGen  mswin   \n\n           Price                        date  \n0         0.0084  2021-11-15 16:40:32.509429  \n2         0.0217  2021-11-15 16:40:32.509429  \n4         0.0341  2021-11-15 16:40:32.509429  \n6         0.0671  2021-11-15 16:40:32.509429  \n8         0.1343  2021-11-15 16:40:32.509429  \n...          ...                         ...  \n16059069  5.9431  2021-12-05 16:46:39.137754  \n16059070  1.5998  2021-12-05 16:46:39.137754  \n16059071  6.0158  2021-12-05 16:46:39.137754  \n16059072  1.5998  2021-12-05 16:46:39.137754  \n16059073  6.0158  2021-12-05 16:46:39.137754  \n\n[8949831 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n      <th>instanceType</th>\n      <th>major</th>\n      <th>minor</th>\n      <th>Type</th>\n      <th>OS</th>\n      <th>Price</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>us-east</td>\n      <td>a1.medium</td>\n      <td>a1</td>\n      <td>medium</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0084</td>\n      <td>2021-11-15 16:40:32.509429</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>us-east</td>\n      <td>a1.large</td>\n      <td>a1</td>\n      <td>large</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0217</td>\n      <td>2021-11-15 16:40:32.509429</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>us-east</td>\n      <td>a1.xlarge</td>\n      <td>a1</td>\n      <td>xlarge</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0341</td>\n      <td>2021-11-15 16:40:32.509429</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>us-east</td>\n      <td>a1.2xlarge</td>\n      <td>a1</td>\n      <td>2xlarge</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0671</td>\n      <td>2021-11-15 16:40:32.509429</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>us-east</td>\n      <td>a1.4xlarge</td>\n      <td>a1</td>\n      <td>4xlarge</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.1343</td>\n      <td>2021-11-15 16:40:32.509429</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16059069</th>\n      <td>eu-south-1</td>\n      <td>m5d.metal</td>\n      <td>m5d</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>mswin</td>\n      <td>5.9431</td>\n      <td>2021-12-05 16:46:39.137754</td>\n    </tr>\n    <tr>\n      <th>16059070</th>\n      <td>eu-south-1</td>\n      <td>r5.metal</td>\n      <td>r5</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>linux</td>\n      <td>1.5998</td>\n      <td>2021-12-05 16:46:39.137754</td>\n    </tr>\n    <tr>\n      <th>16059071</th>\n      <td>eu-south-1</td>\n      <td>r5.metal</td>\n      <td>r5</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>mswin</td>\n      <td>6.0158</td>\n      <td>2021-12-05 16:46:39.137754</td>\n    </tr>\n    <tr>\n      <th>16059072</th>\n      <td>eu-south-1</td>\n      <td>r5d.metal</td>\n      <td>r5d</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>linux</td>\n      <td>1.5998</td>\n      <td>2021-12-05 16:46:39.137754</td>\n    </tr>\n    <tr>\n      <th>16059073</th>\n      <td>eu-south-1</td>\n      <td>r5d.metal</td>\n      <td>r5d</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>mswin</td>\n      <td>6.0158</td>\n      <td>2021-12-05 16:46:39.137754</td>\n    </tr>\n  </tbody>\n</table>\n<p>8949831 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('spotData.csv')\n",
    "raw_data.drop(raw_data[raw_data.Price==\"N/A*\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "          Region instanceType     major     minor               Type  \\\ncount   16059120     16059120  16059120  16059120           16059120   \nunique        22          420        68        20                  9   \ntop      us-east    a1.medium      c6gd    xlarge  generalCurrentGen   \nfreq      729960        38236    344124   2179452            4855972   \n\n              OS     Price                        date  \ncount   16059120  16059120                    16059120  \nunique         2      4143                         869  \ntop        linux      N/A*  2021-11-15 16:40:32.509429  \nfreq     8029560   7109289                       18480  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n      <th>instanceType</th>\n      <th>major</th>\n      <th>minor</th>\n      <th>Type</th>\n      <th>OS</th>\n      <th>Price</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>16059120</td>\n      <td>16059120</td>\n      <td>16059120</td>\n      <td>16059120</td>\n      <td>16059120</td>\n      <td>16059120</td>\n      <td>16059120</td>\n      <td>16059120</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>22</td>\n      <td>420</td>\n      <td>68</td>\n      <td>20</td>\n      <td>9</td>\n      <td>2</td>\n      <td>4143</td>\n      <td>869</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>us-east</td>\n      <td>a1.medium</td>\n      <td>c6gd</td>\n      <td>xlarge</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>N/A*</td>\n      <td>2021-11-15 16:40:32.509429</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>729960</td>\n      <td>38236</td>\n      <td>344124</td>\n      <td>2179452</td>\n      <td>4855972</td>\n      <td>8029560</td>\n      <td>7109289</td>\n      <td>18480</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d) * (876 - 7) - len(raw_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = raw_data.astype({\n",
    "    'Region': 'category',\n",
    "    'instanceType': 'category',\n",
    "    'major': 'category',\n",
    "    'minor': 'category',\n",
    "    'Type': 'category',\n",
    "    'OS': 'category',\n",
    "    'date': 'datetime64'\n",
    "})\n",
    "dates = {v: k for k, v in enumerate(data['date'].drop_duplicates().sort_values())}\n",
    "data['time_idx'] = data['date'].apply(lambda x: dates[x])\n",
    "# data['Price'] = pd.to_numeric(raw_data['Price'], errors='coerce').fillna(-1)\n",
    "data['Price'] = pd.to_numeric(raw_data['Price'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "              Region instanceType major    minor               Type     OS  \\\n0            us-east    a1.medium    a1   medium  generalCurrentGen  linux   \n2            us-east     a1.large    a1    large  generalCurrentGen  linux   \n4            us-east    a1.xlarge    a1   xlarge  generalCurrentGen  linux   \n6            us-east   a1.2xlarge    a1  2xlarge  generalCurrentGen  linux   \n8            us-east   a1.4xlarge    a1  4xlarge  generalCurrentGen  linux   \n...              ...          ...   ...      ...                ...    ...   \n16059069  eu-south-1    m5d.metal   m5d    metal    hiMemCurrentGen  mswin   \n16059070  eu-south-1     r5.metal    r5    metal    hiMemCurrentGen  linux   \n16059071  eu-south-1     r5.metal    r5    metal    hiMemCurrentGen  mswin   \n16059072  eu-south-1    r5d.metal   r5d    metal    hiMemCurrentGen  linux   \n16059073  eu-south-1    r5d.metal   r5d    metal    hiMemCurrentGen  mswin   \n\n           Price                       date  time_idx  \n0         0.0084 2021-11-15 16:40:32.509429         0  \n2         0.0217 2021-11-15 16:40:32.509429         0  \n4         0.0341 2021-11-15 16:40:32.509429         0  \n6         0.0671 2021-11-15 16:40:32.509429         0  \n8         0.1343 2021-11-15 16:40:32.509429         0  \n...          ...                        ...       ...  \n16059069  5.9431 2021-12-05 16:46:39.137754       868  \n16059070  1.5998 2021-12-05 16:46:39.137754       868  \n16059071  6.0158 2021-12-05 16:46:39.137754       868  \n16059072  1.5998 2021-12-05 16:46:39.137754       868  \n16059073  6.0158 2021-12-05 16:46:39.137754       868  \n\n[8949831 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n      <th>instanceType</th>\n      <th>major</th>\n      <th>minor</th>\n      <th>Type</th>\n      <th>OS</th>\n      <th>Price</th>\n      <th>date</th>\n      <th>time_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>us-east</td>\n      <td>a1.medium</td>\n      <td>a1</td>\n      <td>medium</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0084</td>\n      <td>2021-11-15 16:40:32.509429</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>us-east</td>\n      <td>a1.large</td>\n      <td>a1</td>\n      <td>large</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0217</td>\n      <td>2021-11-15 16:40:32.509429</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>us-east</td>\n      <td>a1.xlarge</td>\n      <td>a1</td>\n      <td>xlarge</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0341</td>\n      <td>2021-11-15 16:40:32.509429</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>us-east</td>\n      <td>a1.2xlarge</td>\n      <td>a1</td>\n      <td>2xlarge</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0671</td>\n      <td>2021-11-15 16:40:32.509429</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>us-east</td>\n      <td>a1.4xlarge</td>\n      <td>a1</td>\n      <td>4xlarge</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.1343</td>\n      <td>2021-11-15 16:40:32.509429</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16059069</th>\n      <td>eu-south-1</td>\n      <td>m5d.metal</td>\n      <td>m5d</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>mswin</td>\n      <td>5.9431</td>\n      <td>2021-12-05 16:46:39.137754</td>\n      <td>868</td>\n    </tr>\n    <tr>\n      <th>16059070</th>\n      <td>eu-south-1</td>\n      <td>r5.metal</td>\n      <td>r5</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>linux</td>\n      <td>1.5998</td>\n      <td>2021-12-05 16:46:39.137754</td>\n      <td>868</td>\n    </tr>\n    <tr>\n      <th>16059071</th>\n      <td>eu-south-1</td>\n      <td>r5.metal</td>\n      <td>r5</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>mswin</td>\n      <td>6.0158</td>\n      <td>2021-12-05 16:46:39.137754</td>\n      <td>868</td>\n    </tr>\n    <tr>\n      <th>16059072</th>\n      <td>eu-south-1</td>\n      <td>r5d.metal</td>\n      <td>r5d</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>linux</td>\n      <td>1.5998</td>\n      <td>2021-12-05 16:46:39.137754</td>\n      <td>868</td>\n    </tr>\n    <tr>\n      <th>16059073</th>\n      <td>eu-south-1</td>\n      <td>r5d.metal</td>\n      <td>r5d</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>mswin</td>\n      <td>6.0158</td>\n      <td>2021-12-05 16:46:39.137754</td>\n      <td>868</td>\n    </tr>\n  </tbody>\n</table>\n<p>8949831 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['date'] <= '2021-12-01']\n",
    "data = data.dropna()\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    },
    {
     "data": {
      "text/plain": "              Region instanceType major    minor               Type     OS  \\\n0            us-east    a1.medium    a1   medium  generalCurrentGen  linux   \n2            us-east     a1.large    a1    large  generalCurrentGen  linux   \n4            us-east    a1.xlarge    a1   xlarge  generalCurrentGen  linux   \n6            us-east   a1.2xlarge    a1  2xlarge  generalCurrentGen  linux   \n8            us-east   a1.4xlarge    a1  4xlarge  generalCurrentGen  linux   \n...              ...          ...   ...      ...                ...    ...   \n14211069  eu-south-1    m5d.metal   m5d    metal    hiMemCurrentGen  mswin   \n14211070  eu-south-1     r5.metal    r5    metal    hiMemCurrentGen  linux   \n14211071  eu-south-1     r5.metal    r5    metal    hiMemCurrentGen  mswin   \n14211072  eu-south-1    r5d.metal   r5d    metal    hiMemCurrentGen  linux   \n14211073  eu-south-1    r5d.metal   r5d    metal    hiMemCurrentGen  mswin   \n\n           Price                       date  time_idx  \n0         0.0084 2021-11-15 16:40:32.509429         0  \n2         0.0217 2021-11-15 16:40:32.509429         0  \n4         0.0341 2021-11-15 16:40:32.509429         0  \n6         0.0671 2021-11-15 16:40:32.509429         0  \n8         0.1343 2021-11-15 16:40:32.509429         0  \n...          ...                        ...       ...  \n14211069  5.9431 2021-12-03 14:46:38.411744       768  \n14211070  1.5998 2021-12-03 14:46:38.411744       768  \n14211071  6.0158 2021-12-03 14:46:38.411744       768  \n14211072  1.5998 2021-12-03 14:46:38.411744       768  \n14211073  6.0158 2021-12-03 14:46:38.411744       768  \n\n[7919931 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n      <th>instanceType</th>\n      <th>major</th>\n      <th>minor</th>\n      <th>Type</th>\n      <th>OS</th>\n      <th>Price</th>\n      <th>date</th>\n      <th>time_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>us-east</td>\n      <td>a1.medium</td>\n      <td>a1</td>\n      <td>medium</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0084</td>\n      <td>2021-11-15 16:40:32.509429</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>us-east</td>\n      <td>a1.large</td>\n      <td>a1</td>\n      <td>large</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0217</td>\n      <td>2021-11-15 16:40:32.509429</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>us-east</td>\n      <td>a1.xlarge</td>\n      <td>a1</td>\n      <td>xlarge</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0341</td>\n      <td>2021-11-15 16:40:32.509429</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>us-east</td>\n      <td>a1.2xlarge</td>\n      <td>a1</td>\n      <td>2xlarge</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.0671</td>\n      <td>2021-11-15 16:40:32.509429</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>us-east</td>\n      <td>a1.4xlarge</td>\n      <td>a1</td>\n      <td>4xlarge</td>\n      <td>generalCurrentGen</td>\n      <td>linux</td>\n      <td>0.1343</td>\n      <td>2021-11-15 16:40:32.509429</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14211069</th>\n      <td>eu-south-1</td>\n      <td>m5d.metal</td>\n      <td>m5d</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>mswin</td>\n      <td>5.9431</td>\n      <td>2021-12-03 14:46:38.411744</td>\n      <td>768</td>\n    </tr>\n    <tr>\n      <th>14211070</th>\n      <td>eu-south-1</td>\n      <td>r5.metal</td>\n      <td>r5</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>linux</td>\n      <td>1.5998</td>\n      <td>2021-12-03 14:46:38.411744</td>\n      <td>768</td>\n    </tr>\n    <tr>\n      <th>14211071</th>\n      <td>eu-south-1</td>\n      <td>r5.metal</td>\n      <td>r5</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>mswin</td>\n      <td>6.0158</td>\n      <td>2021-12-03 14:46:38.411744</td>\n      <td>768</td>\n    </tr>\n    <tr>\n      <th>14211072</th>\n      <td>eu-south-1</td>\n      <td>r5d.metal</td>\n      <td>r5d</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>linux</td>\n      <td>1.5998</td>\n      <td>2021-12-03 14:46:38.411744</td>\n      <td>768</td>\n    </tr>\n    <tr>\n      <th>14211073</th>\n      <td>eu-south-1</td>\n      <td>r5d.metal</td>\n      <td>r5d</td>\n      <td>metal</td>\n      <td>hiMemCurrentGen</td>\n      <td>mswin</td>\n      <td>6.0158</td>\n      <td>2021-12-03 14:46:38.411744</td>\n      <td>768</td>\n    </tr>\n  </tbody>\n</table>\n<p>7919931 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_encoder_length = 768\n",
    "max_prediction_length = 100\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "print(training_cutoff)\n",
    "dateCutOff = data[lambda x: x.time_idx <= training_cutoff]\n",
    "dateCutOff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danibondar/opt/miniconda3/lib/python3.9/site-packages/pytorch_forecasting/data/encoders.py:618: UserWarning: scale is below 1e-7 - consider not centering the data or using data with higher variance for numerical stability\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "\n",
    "max_encoder_length = 768\n",
    "max_prediction_length = 100\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length  # time for cutoff\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],  #data[data['date'] <= training_cutoff]\n",
    "    time_idx='time_idx',  # column name of time of observation\n",
    "    target='Price',  # column name of target to predict\n",
    "    group_ids=['Region', 'instanceType', 'Type', 'OS'],  # column name(s) for timeseries IDs\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=['Region', 'instanceType', 'Type', 'OS'],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=['Price'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['Region', 'instanceType', 'Type', 'OS'], transformation=\"softplus\"\n",
    "    ),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "GroupNormalizer(groups=['Region', 'instanceType', 'Type', 'OS'],\n                transformation='softplus')"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.target_normalizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -2.4435e-01,  8.4372e-23, -1.7298e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [ 0.0000e+00, -2.4435e-01,  8.4372e-23, -1.7253e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [ 0.0000e+00, -2.4435e-01,  8.4372e-23, -1.7208e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -5.3104e-01,  8.4372e-23,  1.7208e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [ 0.0000e+00, -5.3104e-01,  8.4372e-23,  1.7253e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [ 0.0000e+00, -5.3104e-01,  8.4372e-23,  1.7298e+00,  0.0000e+00,\n",
      "          0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(training.data['reals'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "# create PyTorch Lighning Trainer with early stopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=0,  # run on CPU, if on multiple GPUs, use accelerator=\"ddp\"\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # 30 batches per epoch\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 58.2k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n",
      "/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/f6/d5b_7znj41s8wv44xjq32jtm0000gn/T/ipykernel_71031/3733913203.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;31m# find the optimal learning rate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 22\u001B[0;31m res = trainer.tuner.lr_find(\n\u001B[0m\u001B[1;32m     23\u001B[0m     \u001B[0mtft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_dataloaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_dataloaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mval_dataloader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mearly_stop_threshold\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1000.0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_lr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m )\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py\u001B[0m in \u001B[0;36mlr_find\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, train_dataloader)\u001B[0m\n\u001B[1;32m    183\u001B[0m         \"\"\"\n\u001B[1;32m    184\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_lr_find\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 185\u001B[0;31m         result = self.trainer.tune(\n\u001B[0m\u001B[1;32m    186\u001B[0m             \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m             \u001B[0mtrain_dataloaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36mtune\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs, train_dataloader)\u001B[0m\n\u001B[1;32m   1092\u001B[0m         )\n\u001B[1;32m   1093\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1094\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtuner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tune\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscale_batch_size_kwargs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mscale_batch_size_kwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr_find_kwargs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlr_find_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1095\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1096\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstopped\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py\u001B[0m in \u001B[0;36m_tune\u001B[0;34m(self, model, scale_batch_size_kwargs, lr_find_kwargs)\u001B[0m\n\u001B[1;32m     51\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_lr_find\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m             \u001B[0mlr_find_kwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetdefault\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"update_attr\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 53\u001B[0;31m             \u001B[0mresult\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"lr_find\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlr_find\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mlr_find_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     54\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTrainerStatus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFINISHED\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/tuner/lr_finder.py\u001B[0m in \u001B[0;36mlr_find\u001B[0;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    237\u001B[0m     \u001B[0;31m# Fit, lr & loss logged in callback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 238\u001B[0;31m     \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtuner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    239\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    240\u001B[0m     \u001B[0;31m# Prompt if we stopped early\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTrainerStatus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mRUNNING\u001B[0m  \u001B[0;31m# last `_run` call might have set it to `FINISHED`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtuning\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1193\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1194\u001B[0m         \u001B[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1195\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1197\u001B[0m         \u001B[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_dispatch\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1273\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_type_plugin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_predicting\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1274\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1275\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_type_plugin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1276\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1277\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrun_stage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001B[0m in \u001B[0;36mstart_training\u001B[0;34m(self, trainer)\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mstart_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"pl.Trainer\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[0;31m# double dispatch to initiate the training loop\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_results\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_stage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mstart_evaluating\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"pl.Trainer\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36mrun_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1283\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredicting\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1284\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_predict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1285\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1286\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1287\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_pre_training_routine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1305\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprogress_bar_callback\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1307\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_sanity_check\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1309\u001B[0m         \u001B[0;31m# enable train mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_sanity_check\u001B[0;34m(self, ref_model)\u001B[0m\n\u001B[1;32m   1369\u001B[0m             \u001B[0;31m# run eval step\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1370\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1371\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_evaluation_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1372\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1373\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"on_sanity_check_end\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    149\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 151\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_run_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    152\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001B[0m in \u001B[0;36mon_run_end\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m         \u001B[0;31m# lightning module method\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 131\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_evaluation_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m         \u001B[0;31m# hook\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001B[0m in \u001B[0;36m_evaluation_epoch_end\u001B[0;34m(self, outputs)\u001B[0m\n\u001B[1;32m    234\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_overridden\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"validation_epoch_end\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    235\u001B[0m                 \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_current_fx_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"validation_epoch_end\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 236\u001B[0;31m                 \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalidation_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    237\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    238\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_on_evaluation_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py\u001B[0m in \u001B[0;36mvalidation_epoch_end\u001B[0;34m(self, outputs)\u001B[0m\n\u001B[1;32m    391\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    392\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mvalidation_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 393\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    394\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    395\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtest_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\u001B[0m in \u001B[0;36mepoch_end\u001B[0;34m(self, outputs)\u001B[0m\n\u001B[1;32m    532\u001B[0m         \"\"\"\n\u001B[1;32m    533\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_interval\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 534\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_interpretation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    535\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    536\u001B[0m     def interpret_output(\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\u001B[0m in \u001B[0;36mlog_interpretation\u001B[0;34m(self, outputs)\u001B[0m\n\u001B[1;32m    746\u001B[0m         \"\"\"\n\u001B[1;32m    747\u001B[0m         \u001B[0;31m# extract interpretations\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 748\u001B[0;31m         interpretation = {\n\u001B[0m\u001B[1;32m    749\u001B[0m             \u001B[0;31m# use padded_stack because decoder length histogram can be of different length\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    750\u001B[0m             \u001B[0mname\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpadded_stack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"interpretation\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mside\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"right\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\u001B[0m in \u001B[0;36mlog_interpretation\u001B[0;34m(self, outputs)\u001B[0m\n\u001B[1;32m    746\u001B[0m         \"\"\"\n\u001B[1;32m    747\u001B[0m         \u001B[0;31m# extract interpretations\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 748\u001B[0;31m         interpretation = {\n\u001B[0m\u001B[1;32m    749\u001B[0m             \u001B[0;31m# use padded_stack because decoder length histogram can be of different length\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    750\u001B[0m             \u001B[0mname\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpadded_stack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"interpretation\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mside\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"right\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/Applications/DataSpell 2021.3.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1147\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/DataSpell 2021.3.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1162\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# define network to train - the architecture is mostly inferred from the dataset, so that only a few hyperparameters have to be set by the user\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    # dataset\n",
    "    training,\n",
    "    # architecture hyperparameters\n",
    "    hidden_size=32,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    # loss metric to optimize\n",
    "    loss=QuantileLoss(),\n",
    "    # logging frequency\n",
    "    log_interval=2,\n",
    "    # optimizer parameters\n",
    "    learning_rate=0.03,\n",
    "    reduce_on_plateau_patience=4\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "\n",
    "# find the optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, early_stop_threshold=1000.0, max_lr=0.3,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# and plot the result - always visually confirm that the suggested learning rate makes sense\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n",
      "/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/f6/d5b_7znj41s8wv44xjq32jtm0000gn/T/ipykernel_71031/501959834.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# fit the model on the data - redefine the model with the correct learning rate if necessary\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m trainer.fit(\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0mtft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_dataloaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_dataloaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mval_dataloader\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m )\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001B[0m\n\u001B[1;32m    735\u001B[0m             )\n\u001B[1;32m    736\u001B[0m             \u001B[0mtrain_dataloaders\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 737\u001B[0;31m         self._call_and_handle_interrupt(\n\u001B[0m\u001B[1;32m    738\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_impl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatamodule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mckpt_path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    739\u001B[0m         )\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    680\u001B[0m         \"\"\"\n\u001B[1;32m    681\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 682\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mtrainer_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    683\u001B[0m         \u001B[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    684\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mexception\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    770\u001B[0m         \u001B[0;31m# TODO: ckpt_path only in v1.7\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    771\u001B[0m         \u001B[0mckpt_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mckpt_path\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 772\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mckpt_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mckpt_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    773\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    774\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstopped\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1193\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1194\u001B[0m         \u001B[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1195\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1197\u001B[0m         \u001B[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_dispatch\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1273\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_type_plugin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_predicting\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1274\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1275\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_type_plugin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1276\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1277\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrun_stage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001B[0m in \u001B[0;36mstart_training\u001B[0;34m(self, trainer)\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mstart_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"pl.Trainer\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[0;31m# double dispatch to initiate the training loop\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_results\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_stage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mstart_evaluating\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"pl.Trainer\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36mrun_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1283\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredicting\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1284\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_predict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1285\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1286\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1287\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_pre_training_routine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1305\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprogress_bar_callback\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1307\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_sanity_check\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1309\u001B[0m         \u001B[0;31m# enable train mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_sanity_check\u001B[0;34m(self, ref_model)\u001B[0m\n\u001B[1;32m   1369\u001B[0m             \u001B[0;31m# run eval step\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1370\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1371\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_evaluation_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1372\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1373\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"on_sanity_check_end\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    149\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 151\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_run_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    152\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001B[0m in \u001B[0;36mon_run_end\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m         \u001B[0;31m# lightning module method\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 131\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_evaluation_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m         \u001B[0;31m# hook\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001B[0m in \u001B[0;36m_evaluation_epoch_end\u001B[0;34m(self, outputs)\u001B[0m\n\u001B[1;32m    234\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_overridden\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"validation_epoch_end\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    235\u001B[0m                 \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_current_fx_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"validation_epoch_end\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 236\u001B[0;31m                 \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalidation_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    237\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    238\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_on_evaluation_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py\u001B[0m in \u001B[0;36mvalidation_epoch_end\u001B[0;34m(self, outputs)\u001B[0m\n\u001B[1;32m    391\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    392\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mvalidation_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 393\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    394\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    395\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtest_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\u001B[0m in \u001B[0;36mepoch_end\u001B[0;34m(self, outputs)\u001B[0m\n\u001B[1;32m    532\u001B[0m         \"\"\"\n\u001B[1;32m    533\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_interval\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 534\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_interpretation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    535\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    536\u001B[0m     def interpret_output(\n",
      "\u001B[0;32m/opt/anaconda3/envs/untitled/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\u001B[0m in \u001B[0;36mlog_interpretation\u001B[0;34m(self, outputs)\u001B[0m\n\u001B[1;32m    749\u001B[0m             \u001B[0;31m# use padded_stack because decoder length histogram can be of different length\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    750\u001B[0m             \u001B[0mname\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpadded_stack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"interpretation\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mside\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"right\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 751\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"interpretation\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    752\u001B[0m         }\n\u001B[1;32m    753\u001B[0m         \u001B[0;31m# normalize attention with length histogram squared to account for: 1. zeros in attention and\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# fit the model on the data - redefine the model with the correct learning rate if necessary\n",
    "trainer.fit(\n",
    "    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "(102361, 23100)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}